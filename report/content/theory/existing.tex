% motivation for distributed storage will be discussed in the introduction
\subsection{Distributed file systems}
Here I will discuss the two most widely used destributed fily systems. We will look at how they work and the implementation. Before I get to that I will use a very basic file sharing system, \ac{nfs}, to illustrate why these distributed systems need their complexity.

\subsubsection*{Network File System}
One way to share files is to expose a filesystems via the network. For this you can use a \textit{shared file system}. These integrate in the files system interface of the client. A widely supported example is \acf{nfs}. In \ac{nfs} a part of a local directory is exported/shared by a local \ac{nfs}-server. Other machines can connect and overlay part of their directory with the exported one. The \ac{nfs} protocol forwards file operations from the client to the host. When an operation is applied on the host the result is traced back to the client. To increase performance the client (almost always) caches file blocks and metadata. 

In a shared envirement it is comman for multiple users to simultaniously access the same files. In \ac{nfs} this can be problamatic. Metadata caching can result in new files appearing up to 30 seconds after they have been created. Furthermore simultaneous writes can become interleaved, writing corrupt data, as each write is turned into multiple network packets~\cite[p. 527]{os}. \ac{nfs} version 4 improves the semantics respecting unix advisory file locks~\cite{rfc3530}. Most applications do not take advisory locks into account concurrent users therefore still risk data corruption. 

\subsubsection*{Google file system}
The \ac{gfs}~\cite{GFS} was developed in 2003 in a response to Googles rapidly growing search index which generated unusually large files~\cite{GFS_interview}. The key to the system is the seperation of the control plane from the data plane. This means that the file data is stored on many \textit{chunk servers} while a single server, the \ac{mds}\footnote{Here I use the term used in \ceph{} for a similar role. \ac{gfs} refers to this as the master}, regulates access to, location of and replication of data. The \ac{mds} also manages file properties. Because all descisions are made on a single machine \ac{gfs} needs no consensus algorithm. A chunk server need not check requests as the \ac{mds} has already done so. 

When a \ac{gfs} client wants to operate on a file it contacts the \ac{mds} for metadata. The metadata includes information on which chunk servers the file content is located. If the client requests to change the data it also recieves which is the primary chunk server. Finally it streams bytes directly to the primary or from the chunk servers. If multiple clients whish to mutate the same file concurrently the primary serializes those requests to some undefined order. See the resulting architecture in \cref{fig:GFS_arch}.
%
\begin{figure}[htbp]
	\centering
	\includesvg[width=1.12\columnwidth, pretex=\scriptsize]{figs/papers/gfs_arch}
	\caption{The \ac{gfs} architecture with the coordinating server, the \ac{gfs} \textit{master}, adopted from~\cite{GFS}.}
	\label{fig:GFS_arch}
\end{figure}
%
When clients mutate multiple chunks of data concurrently and the mutations share one or more chunks the result are undefined. Because the primary chunkserver serializes operations on the chunk level mutations of multiple clients will be interspersed. For example if the concurrent writes of client \textit{A} and \textit{B} translate to mutating chunks $1_a$ $2_a$ $3_a$ for \textit{client A} and $2_b$ $3_b$ for \textit{client B}. The primary could pick serialization: $1_a$ $2_a$ $2_b$ $3_b$ $3_a$. The writes of \textit{A} and \textit{B} have now been interspersed with eachoter. This is a problem when using \ac{gfs} to collect logs. As a solution \ac{gfs} offers atomic appends, here the primary picks the offset at which the data is written. By tracking the length of each append the primary assures none of them overlap. The client is returned the offset the primary picked.

To ensure data will not get corrupted by hardware failure the data is checksummed and replicated over multiple servers. The replicas are carefully spread around to cluster to prevent a network switch or power supply failure taking all replicas offline and to ensure equal utilization resources. The \ac{mds} re-creates lost chuncks as needed. The cluster periodically rebalances chunks between machines filling up newly added servers. 

A single machine can efficiently handle all file metadata requests, as long as files are large. If the cluster grows sufficiently large while the files stay small the metadata will no longer fit in the coordinating servers memory. Effectively \ac{gfs} has a limit on the number of files. This limit became a problem as it was used for services with smaller files. To work around this applications packed smaller files together before submitting the bundle as a single file to \ac{gfs}~\cite{GFS_interview}.

\subparagraph*{Hadoop FS} \label{sec:hdfs}
When Hadoop, a framework for distributed data processing, needed a filesystem Apache developed the \ac{hdfs}~\cite{hdfs}. It is based on the \ac{gfs} architecture, open source and (as of writing) actively worked on. While it kept the file limit it offers improved availibility.

The single \ac{mds}\footnote{\ac{hdfs} refers to it as the namenode} is a single point of failure in the original \ac{gfs} design. If it fails the file system will be down and worse if its drive fails all data is lost. To solve this \ac{hdfs} adds standby nodes that can take the place of the \ac{mds}. These share the \ac{mds}'s data using either shared storage~\cite{hdfs_ha_nfs} (which only moves the point of failure) or using a cluster of \textit{journal nodes}~\cite{hdfs_ha_q} which use a quorum to maintain internal consensus under faults. 

Around 90\% of metadata requests are reads~\cite{hdfs_ha_reads} in \ac{hdfs} these are sped up by managing reads from the standby nodes. The \ac{mds} shares metadata changes with the journal cluster. The standby nodes update via the journal nodes. They can lag behind the \ac{mds}, which breaks consistency. Most notably \textit{read after write}: a client that wrote data tries to reads back what it did, the read request is send to a standby node, it has not yet been updated with the metadata change from the \ac{mds}. The standby node awnsers with the wrong metadata, possibly denying the file exists at all. 

\ac{hdfs} solves this using \textit{coordinated reads}. The \ac{mds} increments a counter on every metadata change change. The counter is included in the response of the \ac{mds} to a write request. Clients performing a \textit{read} include the lastest counter they got. A standby node will hold a read request until the nodes metadata is up to date with the counter included in the request. In the scenario where two clients communicate via a third channel consistency can be achieved by explicitly requesting up to date metadata. The standby node then checks with the \ac{mds} if it is up to date.

\subsubsection*{Ceph} \label{sec:ceph}
Building a distributed system that scales, that is performance stays the same as capacity increases, is quite the challange. The \ac{gfs} architecture is limited by the \acf{mds}. \ceph{}~\cite{ceph} minimizes central coordination enabling it to scale infinitly. Metadata is stored on multiple \ac{mds} instead of a single machine and needs not track where data is located. Instead objects are located using \ceph{}'s defining feature: \emph{\ac{crush}}, a controlable hash algorithm. Given an \textit{innode} number and map of the \ac{obs} \ceph{} uses \emph{\ac{crush}} to locate where a files data is or should be stored. 

A client resolves a path to an \textit{innode} by retrieving metadata from the \ac{mds} cluster. It can scales as needed. Data integrity is achieved without need for central coordination as \acp{obs} compare replicas directly. 

\subparagraph{File Mapping}
Lets take a closer look at how \ceph{} uses \ac{crush} to map a file to object locations on different servers. The process is illustrated in \cref{fig:ceph_crush}. Similar to \ac{gfs} files are first split into fixed size piecesor objects\footnote{\ac{gfs} called these chunks} each is assigned an id based on the files \textit{innode} number. These object ids are hashed into \acp{pg}. \ac{crush} outputs a list of $n$~\acp{osd} on which an object should be placed given a placement group, cluster map and replication factor~$n$. The cluster map not only lists the \acp{osd} but also defines failure domains, such as servers sharing a network switch. \ac{crush} uses the map to minimize the chance all replicas are taken down by part of the infrastructure failing.

\begin{figure}[htbp]
	\centering
	\includesvg[width=0.8\columnwidth, pretex=\scriptsize]{figs/papers/ceph_crush}
	\caption{How \ceph{} stripes a file to objects and distributes these to different machines.}
	\label{fig:ceph_crush}
\end{figure}

The use of \ac{crush} reduces the amount of work for the \acp{mds}. They only need to manage the namespace and need not bother regulating where objects are stored and replicated.

\subparagraph{Capabilities}
File consistency is enforced using capabilities. Before a client will do anything with file content it requests these from a \acp{mds}. There are four capabilities: \textit{read}, \textit{cache} \textit{reads}, \textit{write} and \textit{buffer writes}. When a client is done it returns the capability together with the new file size. A \ac{mds} can revoke capabilities as needed if a client was writing this forces the client to return the new file size if it wrote to the file. Before issuing the \textit{write} capability for a file a \ac{mds} needs to revoke all \textit{cache read} capabilities for that file. If it did not a client caching reads would 'read' stale data from its cache not noticing the file has changed. A \ac{mds} also revoke capabilities to provide correct metadata for file being written to. This is nessesary as the \ac{mds} only learns about the current file upon response of the writer.

% TODO nodes or servers get consistent on terminology
\subparagraph{Metadata}
The \ac{mds} cluster maintains consistancy while resolving paths to innodes, issuing capabilities and providing access to file metadata. Issuing write capabilities for an innode or changing its metadata can only be done  by a unique \ac{mds}, the innodes authoritative \ac{mds}. In the next section we will dicuss how innodes are assigned an authoritative \ac{mds}. The authoritative \ac{mds} additionally maintains cache coherency with other \acp{mds} that cache information for the innode. These other \acp{mds} issue read capabilities and handle metadata reads.

The \ac{mds} cluster must be able to recover from crashes. Changes to metadata are therefore journaled to \ceph{}'s \acfp{osd}. Journalling, appending changes to a log, is faster then updating an on disk state of the system. When a \ac{mds} crashes the \ac{mds} cluster reads through the journal applying each change to recover the state. Since \acp{osd} are replicated metadata can not realistically be lost.

\subparagraph{Subtree partitioning}
If all innodes could share the same authoritative \ac{mds} metadata changes and write capability requests would bottleneck \ceph{}. Instead we group innodes based on their position in the file system hierachy. These groups, each a subtree of the file system, are all assigned their own authoritative \ac{mds}. The members of a group, representing a subtree, dynamically adjust to balance load. The most popular subtrees are split and those hardly taking any load are merged.

To determine the popularity of their subtree each authoritative \ac{mds} keeps a counter for each of their innodes. The counters decayes exponantially with time. It is are increased whenever the corrosponding innode or one of its decendents is used. Periodically all subtrees are compared to decide which to merge and which to split.

Since servers can crash at any time migrating innodes for splitting and merging needs to be performed carefully. First the journal on the new \ac{mds} is appended, noting a migration is in progress. The metadata to be migrated is now appended to the new \ac{mds}'s journal. When the transfer is done an extra entry in both the migrated to and migrated from server marks completion and the transfer of authority. 
