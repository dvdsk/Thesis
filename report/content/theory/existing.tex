% motivation for distributed storage will be discussed in the introduction
\subsection{Distributed file systems}
Here I will discuss the two most widely used destributed fily systems. We will look at how they work and the implementation. Before I get to that I will use a very basic file sharing system, \ac{nfs}, to illustrate why these distributed systems need their complexity.

\subsubsection*{Network File System}
A basic way to share files is to expose a filesystems via the network. For this you can use a \textit{shared file system}. These integrate in the interface of the client. A widely supported system is \acf{nfs}. In \ac{nfs} a part of a local directory is exported/shared by a local \ac{nfs}-server. Other machines can then connect and overlay part of their directory with the exported one. The \ac{nfs} protocol forwards file operations from the client to the host over the network. When an operation has been applied on the host the result is traced back to the client. To increase performance the client (almost always) caches file blocks and metadata. 

In a shared envirement it is commanplace for multiple users to simultaniously access the same files. In \ac{nfs} this can be problamatic, as meta data is cached new files can appear to other users after 30 seconds. Further more simultaneous writes can become interleaved as each write gets split into multiple network packets~\cite[p. 527]{os}, writing corrupt data. Version 4 improves the semantics respecting unix advisory file locks~\cite{rfc3530}. Most applications do not take advisory locks into account still risking data corruption. 

\subsubsection*{Google file system}
The Google File System~\cite{GFS} was developed in 2003 in a response to Googles rapidly growing search index which generated unusually large files~\cite{GFS_interview}. The key to the system is the seperation of the control plane from the data plane. That is, the file data is stored on many \textit{chunk servers} while a single server, the coordinator, regulates access to, location of and replication of data. The coordinator also serves the metadata. Because all descisions are made on a single machine \ac{gfs} needs no consensus algorithm. A chunk server simply executes all client requests as the coordinator will already have decided if the request is allowed. 

When a \ac{gfs} client wants to operate on a file it contacts the main node for metadata. The client then uses the metadata to determine on which chunk servers the file content is located. If it requests to change the data it also recieves which is the primary chunk server. Finally it streams bytes directly to the primary or from the chunk servers. If multiple clients whish to mutate the same file concurrently the primary serializes those requests to some undefined order. See the resulting architecture in \cref{fig:GFS_arch}.
%
\begin{figure}[htbp]
	\centering
	\includesvg[width=1.12\columnwidth, pretex=\scriptsize]{figs/papers/gfs_arch}
	\caption{The \ac{gfs} architecture with the coordinating server, the \ac{gfs} \textit{master}, adopted from~\cite{GFS}}
	\label{fig:GFS_arch}
\end{figure}
%
When clients mutate multiple chunks of data concurrently and the mutations share one or more chunks the result will be undefined. Because primary chunkserver serializes operations on the chunk level the mutations of multiple clients will be interspersed. For example the concurrent writes of client \textit{A} and \textit{B} translate to mutating chunks $1_a$ $2_a$ $3_a$ for \textit{client A} and $2_b$ $3_b$ for \textit{client B}. The primary could pick serialization: $1_a$ $2_a$ $2_b$ $3_b$ $3_a$. The writes of \textit{A} and \textit{B} have now been interspersed with eachoter. This can become a problem when using \ac{gfs} to collect logs. As a solution \ac{gfs} offers atomic appends, here the primary picks the offset at which the data is written. By tracking the length of each append the primary assures none of them overlap. The client is returned the offset the primary picked.

To ensure data does not get corrupted it is checksummed and replicated over multiple servers. The replicas are carefully spread around to cluster to prevent a network switch or power supply failure taking all replicas offline and to ensure equal utilization resources. The coordinating server re-creates lost chuncks as needed. The cluster periodically rebalances chunks between machines filling up newly added servers. 

A single machine can efficiently handle all file metadata requests, as long as files are sufficiently large. If the cluster grows sufficiently large while the files stay small the metadata will no longer fit in the coordinating servers memory. Effectively \ac{gfs} has a limit on the number of files. This limit became a problem as it was used for services with smaller files. To work around this applications packed smaller files together before submitting the bundle as a single file to \ac{gfs}~\cite{GFS_interview}.

\subparagraph*{Hadoop FS}
When Hadoop, a framework for distributed data processing, needed a filesystem Apache developed the \ac{hdfs}~\cite{hdfs}. It is based on the \ac{gfs} architecture, open source and, as of writing, actively worked on. While it still suffers from the file limit it offers improved availibility.

The single coordinating server is a single point of failure in the design. If it fails the file system will be down and worse if its drive fails all data is lost. This limits the use of \ac{hdfs} to applications. To solve this \ac{hdfs} adds standby nodes that can take the place of the metadata node. These share the coordinators data either using shared storage~\cite{hdfs_ha_nfs}, which only moves the point of failure, or using a cluster of \textit{journal nodes}~\cite{hdfs_ha_q} which use a quorum to maintain internal consensus under faults. 

Around 90\% of metadata requests are reads~\cite{hdfs_ha_reads}, therefore \ac{hdfs} can serve reads from the standby nodes. The coordinator shares metadata changes with the journal cluster. Because the standby nodes update via the journal nodes they can lag behind the coordinator. This breaks breaks consistency notably \textit{read after write}: a client that wrote data tries to reads back what it did, the read request is send to a standby node, it has not yet been updated with the metadata change from the coordinator and awnsers with the wrong metadata, possibly denying the file exists at all. 

\textsc{hdfs} provides consistency using \textit{coordinated reds}. The coordinator trackes the state of the metadata, incrementing a counter on every change. The counter is included in the coordinaters response to a write. Clients performing a \textit{read} include the lastest counter they got. A standby node will hold the request until its metadata is up to date with the counter. In the scenario where two clients communicate via a third channel consistency can be achieved by explicitly requesting up to date metadata. The standby node then checks wit the coordinator if it is up to date.

%TODO formatting \textsc might not be optimal
%TODO naming, uses ceph for everything (rename GFS/Hadoop) and add footnotes listing the GFS/Hadoop names
\subsubsection*{Ceph}
Building a distributed system that scales, that is performance stays the same as capacity increases, is quite the challange. The \ac{gfs} architecture is limited by the metadata serving coordinator. \n{Ceph}~\cite{ceph} minimizes central coordination enabling it to scale infinitly. Metadata is stored on multiple \ac{mds} instead of a single machine and needs not track data distribution. Instead objects are located using \n{ceph}'s defining feature: \emph{\ac{crush}}, a controlable hash algorithm. Given an innode number and map of the object stores \n{ceph} uses the \emph{\ac{crush}} algorithm to locate where a files data is stored. 

enables clients to locate data needing only the \textit{innode number} instead of a metadata server tracking data distribution. Given a path metadata such as the innode is be retrieved from the \textit{metadata cluster}. It can scales linearly with cluster load. Data integrity is also achieved without need for central coordination. Finally \n{ceph} charts the entire system with a growable \textit{monitor cluster}. This is enabled by \n{ceph}s defining feature: the \emph{\ac{crush}} algorithm which given an innode number outputs a list of nodes on which the data is or should be stored.

\subparagraph{File Mapping}
Lets take a closer look at how \n{ceph} uses hashing to map a file to object locations on different servers. The process is illustrated in \cref{fig:ceph_crush}. Similar to \ac{gfs} files are first split into fixed size objects, each object is assigned an id based on the files innode number. These object ids are hashed into \acp{pg} . \ac{crush} outputs a list of $n$ nodes on which an object should be placed given a placement group, cluster map and replication factor $n$ . The cluster map not only lists all the object stores but defines failure domains, such as servers sharing a network switch. \ac{crush} uses the map to minimize the chance all replicas are taken down by one failure.

\begin{figure}[htbp]
	\centering
	\includesvg[width=0.8\columnwidth, pretex=\scriptsize]{figs/papers/ceph_crush}
	\caption{How \n{ceph} stripes a file to objects and distributes these to different machines}
	\label{fig:ceph_crush}
\end{figure}

The use of \ac{crush} reduces the amount of work for the metadata servers. They only need to manage the namespace and need not bother regulating where objects are stored and replicated.

\subparagraph{Capabilities}
File consistency is enforced using capabilities. Before a client will do anything with a files content it requests these from the \ac{mds} (metadata servers). There are four capabilities: \textit{read}, \textit{cache} \textit{reads}, \textit{write} and \textit{buffer writes}. When a client is done writing it returns the capability together with the new file size. The \ac{mds} can revoke capabilities as needed, in the case of write forcing the client to return the new size. For example before issuing the \textit{write} capability for a file the \ac{mds} needs to revoke all \textit{cache reads} capabilities for that file. If it did not a client caching reads would 'read' stale data from its cache while not noticing the file has changed. The \ac{mds} also revoke capabilities to provide correct metadata about a file being written to such as when its file size is requested.

% TODO nodes or servers get consistent on terminology
% TODO write about journal location (RADOS)
\subparagraph{Metadata}
Clients need an innode number and capabilities before they can find and access file data. This is handled by the metadata server cluster. These maintain consistancy while resolving paths to innodes, issuing capabilities and providing access to file metadata. Issuing write capabilities for an innode or changing its metadata can only be done  by a single node, the innodes authoritative \ac{mds}. In the next section we will dicuss how innodes are assigned an authoritative \ac{mds}. The authoritative \ac{mds} additionally maintains cache coherency with other \ac{mds} that cache information for the innode. These other \ac{mds} are used to issue read capabilities and read the innodes metadata.

To recover from crashes changes to metadata are journaled to shared storage. Journalling, appending changes to a log, is faster then storing the updated state of the system. When a node crashes the \ac{mds} cluster reads through the journal to recover the state.

\subparagraph{Subtree partitioning}
If all innodes would share the same authoritative \ac{mds} metadata changes and write capability requests would bottleneck \n{ceph}. This would be similar to \ac{hdfs} with standby nodes. Instead we group innodes based on their position in the file system hierachy. Each of these groups, each a subtree of the file system, are assigned their own authoritative \ac{mds} balancing the load. The subtrees adapt to the load on the system. The most popular subtrees are split and those responsible for low load are merged.

To determine the popularity each authoritative \ac{mds} track the populatity of their subtree by tracking a counter for each innode. The counters decayes exponantially with time. They are increased whenever the corrosponding innode or one of its decendents is used. Periodically all subtrees are compared migrating part the most popular subtrees to their own authoritative \ac{mds} while merging less popular subtrees.

Since servers can crash at any time migration needs to be performed carefully. First the journal on the new \ac{mds} is appended to noting a migration is in progress. The metadata to be migrated is now appended to the new \ac{mds}'s journal. When the transfer is done an extra entry in both the migrated to and migrated from server marks completion and the transfer of authority. 
