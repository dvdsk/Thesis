\subsection{Distributed Computing}
When state of the art hardware is no longer fast enough to run a system the only option is scaling out. Then there is a choice, do you buy expansive, reliable high performance supercomputer or commodity servers connected by Ip and ethernet? This is the choice between High Performance (HPC) and Distributed Computing. With HPC faults in the hardware are rare and can be handled by restarting, easing development. In a distributed context faults are the norm, restarting the entire system is not an option or you would be down all the time. Resiliance against faults comes at an, often significant, cost to performance. It may also limit scalability. As the scale of a system increases so does the frequancy with which one of the parts fails. Even the most robust part will fail and given enough of them the system will fail frequantly. Therefore at very large scales HPC is not even an option. 
