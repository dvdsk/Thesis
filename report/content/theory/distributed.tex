Here we discuss what distributed computing is and when we use it. How faults and delays make it challenging to build simultaneously consistent and highly available systems. Then we look at consensus algorithms that solve those challenges. After this we discuss what a file system is before we go over the most widely used distributed file systems: HDFS and Ceph. Finally, we look at work related to this thesis.

\subsection{Distributed Computing}
When state-of-the-art hardware is no longer fast enough to run a system the option that remains is scaling out. Here then there is a choice, do you use an expansive, reliable high performance supercomputer or commodity servers connected by IP and Ethernet? This is the choice between \acf{hpc} and Distributed Computing. With HPC faults in the hardware are rare and can be handled by restarting, simplifying software. In a distributed context faults are the norm, restarting the entire system is not an option as you would be down all the time. Resilience against faults comes at an often significant, cost to performance. Fault tolerance may limit scalability. As the scale of a system increases so does the frequency with which one of the parts fails. Even the most robust part will fail and given enough of them the system will fail frequently. Therefore, at very large scales HPC is not even an option. 
