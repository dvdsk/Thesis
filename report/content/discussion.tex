In the previous section we presented the performance of \name{}. We focussed on its two distinguishing features: the \textit{ministry architecture} and its \textit{ranged based file locking}. Another important characteristic of any system is its complexity. The main contribution of the \raft{} consensus algorithm is a more understandable solution that is therefore easier to implement. We begin by discussing the complexity of \name{} then we discuss the performance implications of the \textit{ministry architecture} before we finish discussing \raft{ranged based file locking}.
%
\subsection{Complexity}
Raft is an understandable consensus algorithm. Here we have extended it to get \textit{Group Raft} which provides greater scalability. This extended algorithm had two problems. These where both solved by re-using the Raft concept of terms. No new variables where added to the algorithm nor did we add new routines. It is still quite simple. 

\Name{} clerks need to know if there log is running behind. For this we added Perishable Log entries to \textit{Group Raft}. These use log entry arrival time, the groups leaders commit index and the heartbeat duration to determine if a log entry is applied on time. Tracking arrival time is a trivial addition. All other variables where already available making perishable a simple addition.

As leases did not need to survive system crashes and reboots we did not use Raft but designed our own algorithm for non-persistent consistency. This enables \name{} to reach higher read and write performance. Even non-persistent consistency is still a hard problem, therefore this algorithm makes \name{}'s implementation significantly more complex. In \cref{sec:impl_leases} we saw there are four known problems with this algorithm. While most have solutions solving these will further increase the now already high degree of complexity.
%
\subsection{Ministry architecture}
\paragraph{List Directory}
We saw the fastest response from a \name{} configuration using only a single ministry. As soon as we start using multiple latency increases. Adding more ministries increases average performance. Clients start without knowledge of the configuration, they are designed to initially connect to a random node. It is therefore logical that we see an increased in latency with more ministries: given twice as many ministries the chance to connect to the right node on the first try halves. 

As expected the request order \textit{batch} order is faster than \textit{stride}. We would however expect there to be a larger difference. We did not expect the difference to get more pronounced as the number of ministries increases. It could be that it takes the client longer to re-establish its connection when the connection has been out longer. This requires study of the clients' performance and implementation.

The tail latencies become longer as we increase the number of ministries. This is unexpected. Requests should arrive at the right ministry after at most one redirect. The number of requests that need a redirect should increase as the number of ministries increases however those redirects should take the same amount of time. An explanation could be that the higher number of redirects overloads the redirect system leading to longer response times for those queries.
%
\paragraph{Create file} 
% TODO: redo plot showing results of individual runs 
%       as different lines with the same color 
% <08-08-22> 
Mean file creation time decreases almost linearly with the number of ministries. This should hold true even after the Raft implementation has been improved. Most file creation complete in a multiple of 75ms. This corresponds to the heartbeat duration used for Raft which imposes a rate limits on log modifications in this implementation. 

Up to 5\% of writes are completed in less time. At least a heartbeat period must pass before a new log entry can be appended. Only after appending the log entry does the minister inform the client the creation is done. These low create durations must therefore be the result of a bug in either the synchronizing of the test results or the file create implementation.
%
\subsection{Ranged based file locking}
\paragraph{Writing a single row}
Locking only the needed data when writing to a file gives a dramatic increase in performance. This is in line with our expectations as the chance of lock contention decreases given less overlap in lock regions. Given larger files the difference in performance between the methods decreases as the time spend on (simulated) IO starts to dominate the time spend on locking. There are some far outliers that only appear when using row locking at the same time the variance of row locking is lower. I have no explanation for this, and it should be studied further starting with the behavior of the client.

In this experiment both methods lock the file once and both locks should succeed in the same time given the files start unlocked. The only difference is the range of the lock that is acquired. Locking the entire file is however much faster. This even holds with a single writer in which case there can be no contention. With only a single writer both methods should therefore do exactly the same and have the same results. Only when using more than four simultaneous writers do we see locking the smaller range becoming faster. Given these results, especially the results when using a single writer, there is a fault in either the test or the lease system.
%
\paragraph{Writing the entire file}
The lower lock contention when locking by row does not make up for the overhead of extra lock operations when locking ten times. Even when using very large files with rows of 80 \ac{mby} locking the entire file is significantly faster. The gap does seem to close as row size increases. There is a uniform distribution of results when locking the entire file. This distribution highlights how each write can only be started after the previous completes. The width of the distribution comes from the five separate runs that are presented.
