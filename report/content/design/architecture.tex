Here we present \name{}'s design and explain how it enables scalable consistent distributed file storage. First we will discuss the \ac{api} exposed by our system then I will present the architecture, finally I will detail some system behavior using state machine diagrams. 
%
\subsection{API and Capabilities}
The file system is set up hierarchically: data is stored in files which are kept in folders. Folders can contain other folders. The hierarchy looks like a tree where each level may have 1 to n nodes.

The \ac{posix} has enabled applications to be developed once for all complying file system and our system's \ac{api} is based on it. If we expand beyond \ac{posix} by adding methods that allow for greater performance we exclude existing applications from these gains. This is a trade-off made by \ceph{}~(\cref{sec:ceph}) by implementing part of the \ac{posix} \ac{hpc} IO extensions~\cite{hpc_posix}. \ceph{} also trades in all consistency for files using the \ac{hpc} \ac{api}. 

We also expand the file system \ac{api} beyond \ac{posix}. While this makes our system harder to use it does not come at a cost of reducing consistency. Specifically \name{} expands upon \ac{posix} with the addition of \textsl{open region}. A client that \textit{opens} a file \textit{region} can access only a range of data in the file. This \ac{api} enables consistent \emph{limited parallel concurrent} writes on, or combinations of reading and writing in the same file.

Like \ceph{} in \name{} clients gain capabilities when they open files. These capabilities determine what actions a client may perform on a file. There are four capabilities: \nopagebreak
%
\begin{multicols}{4} % 2 does not work, anyway this way the list is nice, tight and centerd
\begin{itemize}
	\item read
	\item read region
	\item write
	\item write region
\end{itemize}
\end{multicols}
%
A client with write capabilities may also read data. Similar to \ceph{} capabilities are only given for a limited time, this means a client is given a lease to certain capabilities. The lease needs to be renewed if the client is not done with its file operations in time. The lease can also be revoked early by the system.

\begin{samepage}
\Name{} tracks the capabilities for each of its files. It will only give out capabilities that uphold the following:
%
\begin{itemize}
	\item Multiple clients can have capabilities on the same file as long as write capabilities have no overlap with any region.
\end{itemize}
\end{samepage}
% 
\subsection{Architecture} \label{sec:arch}
\begin{figure}
	\input{figs/diagrams/arch.tex}
	\caption{An overview of the architecture. There is one president, $n$ \emph{ministers}~\amdsLeg{}, each ministry can have a different number of \emph{clerks}~\cmdsLeg{}. Not all servers in a cluster are always actively used as represented by the \emph{idle} nodes~\umdsLeg{}}
\end{figure}

\Name{} uses hierarchical leadership, there is one president elected by all servers. The president in turn appoints multiple ministers then assigns each a group of clerks. A minister contacts its group, and promotes each member from idle to clerk, forming a ministry. 

The president coordinates the cluster: monitoring the population, assigning new ministers on failures, adjusting groups given failures and load balances between all groups. Load balancing is done in two ways: increasing the size of a group and increasing the number of groups. To enable the president to make load balancing decisions each minister periodically sends file popularity.

Metadata changes are coordinated by ministers, they serialize metadata modifying requests and ensures the changes proliferate to the ministry's clerks. Changes are only completed when they are written to half of the clerks. Each minister also collects file popularity by querying its clerks periodically. Finally, write capabilities can only be issued by ministers.

A ministry's clerks handle metadata queries: issuing read capabilities and providing information about the file system tree. Additionally, each clerk tracks file popularity to provide to the minister. 

It is not a good idea to assign as many clerks to ministries as possible. Each extra clerk is one more machine the minister needs to contact for each change. The cluster might therefore keep some nodes idle. We will get back to this when discussing load balancing in \Cref{sec:loadb}.
%
\subsubsection*{Consensus} \label{sec:concensus} \label{sec:praft}
Consistent communication has a performance penalty and complicates system design. Not all communication is critical to system or data integrity. We use simpler protocols where possible. 

The president is elected using \raft{}. Its coordination is communicated using \raft{}'s log replication. On failure a new president is elected by all nodes.

Communication from the minister to its ministries clerks uses log replication similar to \raft{}. When a minister is appointed it gets a \raft{} term. Changes to the metadata are shared with clerks using log replication. A minister can fail after the change was committed but before the client was informed of the success. The log index is shared with the client before it is committed, on minister failure the client can check with a clerk to see if its log entry exists\footnote{Without the log index the client can not distinguish between failure and a successful change followed by a new change overriding the clients.}. When the minister fails the president selects the clerk with the highest \textsl{commit index} as the new minister. We call this extension of Raft \textit{Group Raft}.

Load reports to the president are sent over TCP, which will almost always ensure the reports arrive in order. A minister that froze, was replaced and starts working again can still send outdated reports. By including the term of the sending minister the president detects outdated reports and discards them.
%
\subsubsection*{\graft{}} \label{sec:dictraft}
In \textit{Group Raft} a third party instructs a node to become the leader. Followers are not informed directly but rather accept the new leader as it has a higher term. When receiving a message the normal \raft{} rules apply therefore messages from the old leader\footnote{The third party replacing the leader usually indicates there is a problem with the current leader, making it doubly important that messages from old leaders are ignored.} are rejected as there term is too low.

Nodes must be able to move between groups with assigned leaders. This present two problems, the first issue becomes apparent looking at the following series of events:
\begin{itemize}
	\item Leader B receives message that follower x is assigned to it
	\item Leader B appends to its log and sends an append request to its followers now including x
	\item Follower x \emph{accepts} the request as x has a \emph{lower} term then the request
	\item Follower x \emph{increases} its term to match B
	\item Leader A receives a message that assigns x back to it
	\item Leader A appends to its log and sends an append request to its followers which now again includes x
	\item Follower x \emph{rejects} the request as x now has a \emph{higher} term then the request
\end{itemize}
Leader A must initially have a lower term then B and then a higher term then B for the re-assignment to work. The second problem: we need a follower to re-write its log after every move to match that of its new group. 

To solve the first problem we make the third party change the term of the leader when it assigning it a node. Every re-assignment will succeed if it guarantees the new term is the highest of all the groups. It is simple to ensure that using a single third party as we can then atomically increment the term every assignment and re-assignment. 

This coincidentally also solves our second problem. Since the highest number is always unique the correct log for each group now has an increasing sequence of unique terms. If a node receives an append request and its previous log entries term and index do not match that of the leader it rejects the request. The leader then starts sending older log entries\footnote{This is optimized in \name{} by clearing the entire log if a clerk came from another ministry.}. A successful append can only happen on correct (partial) group log given terms are now unique to groups.
