\subsection{Client requests} %TODO rename leases -> capabilities or get clear on what the difference is
Here we go over all the operations of the system while discussing four in greater detail. I also discuss how other operations are simplere forms of these four. This section is split in two parts: client requests and coordination by the president.
%
For all requests a client needs to find a \ac{mds} to talk to. If the request modifies the namespace, such as write, create and delete, the client needs to contact the \ac{amds}. In \Cref{fig:find_aMDS} we see how this works. Since load balancing changes and \ac{amds} appointments are communicated through \raft{} each cluster member knows which \ac{mds} group and \ac{amds} owns a given subtree. Because client does not know the current \raft{} \textit{term} nor \textit{commit} they can not judge if the information is up to date. At worst this means a few extra jumps through irralivent \acp{mds} before they get up to date information.
%
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/find_aMDS.tex}
	\caption{A new client~\clientLeg{} finding an \ac{amds}~\amdsLeg{} for a file. Its route can go through the wrong subtree~\umdsLeg{} or via a group member~\cmdsLeg{}. Whenever there is a choice dotted lines indicate the less likely path.}
	\label{fig:find_aMDS}
\end{figure}
%
\subsection*{Capabilities} 
A client needing write capability on a file contacts the \ac{amds}. It checks if the lease can be given out and asks its \acp{cmds} to revoke outstanding read leases that conflict. A read lease conflict when its region overlaps with the region needed for the write capability. If a \ac{cmds} lost contact with a client it can not revoke the lease and has to wait till the lease expires. The process is illustrated in \Cref{fig:write}. 

For read capabilities the requests is send to a \ac{cmds}. It checks against the \raft{} log if the leases would conflict with an outstanding write lease. If no conflict is found the lease is issued and the connection to the client kept active. Keeping an active connection makes it possible to revoke or quickly refresh the lease.
%
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/write.tex}
	\caption{A client~\clientLeg{} requesting ranged write capabilities. It finds and contacts the \ac{amds}~\amdsLeg{}. The \ac{amds} then contacts the \acp{cmds}~\cmdsLeg to clear conflicting read capabilities. Meanwhile it waits for any conflicting write leases it gave out to expire.}
	\label{fig:write}
\end{figure}
%
\subsection*{Namespace Changes}
Most changes to the namespace are simpel edits to a \ac{mds} groups metadata table. The client sends its request to the \ac{amds}. The change is performed by adding a commands to the \ac{praft} log (see: \Cref{sec:praft}). Before committing the change the client gets the log index for the change. If the \ac{amds} goes down before acknowleding success the client verifies if the change happend using the log index.

Removing a directory spanning one or more load balanced subtrees needs a little more care. One or more \acp{amds} will have to delete their entire subtree. This requires coordination across the entire cluster. The clients remove requests is forwarded by the \ac{amds} to the \textit{president}. It in turn appends \textsl{Subtree Delete} to the cluster wide log. The client recieves the log index for the command to verify success even if the \textit{president} fails. The steps the \ac{amds} takes are shown in \Cref{fig:rm}. 
%
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/del_dir.tex}
	\caption{An \ac{amds}~\amdsLeg{}, here $\ac{amds}_j$, removes a directory (tree) that is load balanced between multiple groups. The president~\presidentLeg{} coordinates the removal by appending a command to the log. Once it is committed the \acp{amds} hosting subtrees of the directory demote themself to \ac{umds} and $\ac{mds}$ group $j$ drops the directory from its db}
	\label{fig:rm}
\end{figure}
%
\subsection{Availibility}
Ensuring file system avalibility is the responsibility of the \textit{president}. This includes replacing nodes that fail and load balancing. All \acp{amds} and \acp{cmds} periodically send heartbeats to the president. In this design I improve scalability by moving tasks from a central authority (the president) to groups of nodes. Following this pattern we could let the \acp{amds} monitor their \acp{cmds}. This is more complex then letting them report directly to the \textit{president}. Even reporting directly to the president is not needed, instead I use the TCP ack to the \textit{president}'s \raft{} heartbeat. When the \textit{president} fails to send a \raft{} heartbeat to a node it decides the node must be failing.

\subsubsection*{A failing \ac{mds}}
When the president notices an \ac{amds} has failed it will try to replace it. It queries the \ac{amds}'s group to find the ideal candidate for promotion. It it gets a response from less then half the group it can not safely replace the \ac{amds}. At that point it marks the file subtree as down and may retry in the future. A succesful replace is illustrated in \Cref{fig:appoint}.

% TODO what if there is no free umds and group gets too small?
% TODO instead wait till load balance check corrects it?
% what to do with nodes quickly going down?
% just check load balance often? (is just calc not the data geathering)
A \ac{cmds} going down is handled by the president in one of two ways:
\begin{itemize}
	\item There is at least one \ac{umds}. The president assigns the \ac{umds} to the failing nodes group. 
	\item There are no free \ac{umds}. The president, through raft, commands a \ac{cmds} in the group with the lowest load to demote itself. Then the president drops the matter The \ac{cmds} wait till its read leases expire and unassigns.
\end{itemize}
When the groups \ac{amds} appends to the groups \ac{praft} log it will notice the replacement \ac{cmds} is outdated and update its log (see \cref{sec:raft}).

\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/apoint.tex}
	\caption{An \ac{amds}~\amdsLeg{} fails and does not send a heartbeat on time (1). The president~\presidentLeg{} requests the latest commit index (2). Node $\text{\ac{cmds}}_1$~\cmdsLeg{} has index $k$ and it is the highest (3). The president has promoted $\text{\ac{cmds}}_1$ to \ac{amds}, it has started sending heartbeats (4). Note the heartbeats send by the \acp{cmds} are not shown.}
	\label{fig:appoint}
\end{figure}
%
\subsubsection*{Load balancing} \label{sec:loadb}
From the point of avalibility a system that is up but drowning in requests might as well be down. To prevent nodes from getting overloaded we actively balance the load between \ac{mds} groups, add and remove groups and expand the read capacity of groups. A load report contains cpu utilization for each node in the group and the popularity of each bit of metadata split into read and write popularity. The President checks the load balance for each report it recieves.
%
\subparagraph*{Tradeoff}
There is a tradeoff here: a larger group means more nodes for the \ac{amds} to communicate changes to, slowing down writes. As the file system is split into more subtrees the chance increases a client makeing a set of changes will need to contact not one but multiple groups. Further more to create another group we ususally have to shrink existing groups. In reverse growing a group can involve removing one to free \acp{mds}. 
%
We can model the read and write capacity of a single group as:
\begin{align}
	r =& n_\text{c} \\
	w =& 1 - \sigma*n_\text{c}
\end{align}%
Here $n_\text{c}$ is the number of \ac{cmds} in the group, and $\sigma$ the communication overhead added by an extra \ac{cmds}. 

Now we can derive the number of \acp{cmds} needed give a load. Since we want to have some unused capacity $\delta$. I set $\delta$ equal to the capacity with the current load subtracted. This gets us the following system of equations: 
\begin{align}
	\delta &= w - W \\
	\delta &= r - R
\end{align}%
Now solve for $n_\text{c}$, the number of \acp{cmds}.
\begin{align}
	r - R &= w - W \\
	n_c - R &= \left(1 - \sigma*n_c\right) - W \\
	n_c &= \frac{R - W + 1}{1 + \sigma}
\end{align}
From this I draw two conclusions, any spare capacity for writing is at a cost of spare reading capacity. The number of \ac{cmds} roughly the read load.
%
% TODO a load balancing algorithm
%
\subparagraph*{Read balancing}
A group expeciencing low read load relative to their capacity are shrunk. A group is under low read load if it could lose a \ac{cmds} without average \ac{cmds} cpu utilization under 85\%. A group has multiple members because not only for performance. The members form a \praft{} cluster and ensure metadata is stored redundantly. This only works if a group has at least three members. To shrink a group the President issues a \raft{} message unassigning a group member. 

A group under high relative read load has average \ac{cmds} cpu utilization passing 95\%. The president then issues a \raft{} message assigning a \ac{umds} to the group if one is availible. %TODO diff % why
%
\subparagraph*{Write balancing}
When the president decides a group can no longer handle the write load it will try to split off some work to another group. If no existing group can handle the write work the president will try to create a new group. If that is not possible then the 



% TODO subdiagram unassign node

\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/subtree.tex}
	\caption{An \ac{amds}~\amdsLeg{} under too high a load, higher then all other, sends a load report to the president~\presidentLeg{}. It can not create a new \ac{mds} group as there are no or not enough \acp{umds}~\umdsLeg{}. The president removes three \acp{cmds}~\cmdsLeg{} from the groups under the lightest read load and queues the load report. After the \ac{cmds} removal is comitted the load report is unqueued. }
	\label{fig:subtree}
\end{figure}
