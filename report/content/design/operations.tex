\subsection{Client requests} %TODO rename leases -> capabilities or get clear on what the difference is
In this section we go over all the operations of the system, while discussing four in greater detail. I also explain how most operations are simpler forms of these four. This section is split in two parts: client requests and coordination by the president.
%
For all requests a client needs to find an official (a minister or clerk) to talk to. If the request modifies the namespace, such as write, create and delete, the client needs to contact the minister. In \Cref{fig:find_aMDS} we see how this works. Since load balancing changes and minister appointments are communicated through \raft{} each cluster member knows which ministry owns a given subtree. A client can not judge whether the node it got directions from has up to date and correct information. In the rare case the directions where incorrect this means an extra jump through an irrelevant ministry before getting correct information.
%
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/find_aMDS.tex}
	\caption{A new client~\clientLeg{} finding the responsible minister~\amdsLeg{} for a file. Its route can go through the wrong subtree~\umdsLeg{} or via the correct ministry's clerk~\cmdsLeg{}. Whenever there is a choice the dotted line indicate the less likely path.}
	\label{fig:find_aMDS}
\end{figure}
%
\subsection*{Capabilities} \label{sec:lease}
A client needing write-capability on a file contacts the minister. It in turn checks if the lease can be given out and asks its clerks to revoke outstanding read leases that conflict. A read lease conflict when its region overlaps with the region needed for the write capability. If a clerk lost contact with a client it can not revoke the lease and has to wait till the lease expires. The process is illustrated in \Cref{fig:write}. 

If the client needs read capabilities it sends its requests to a clerk. The clerk checks against the \raft{} log if the leases would conflict with an outstanding write lease. If no conflict is found the lease is issued and the connection to the client kept active. Keeping an active connection makes it possible to revoke the lease or quickly refresh it.
%
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/write.tex}
	\caption{A client~\clientLeg{} requesting ranged write capabilities. It finds and contacts the responsible minister~\amdsLeg{}. The minister then contacts the ministry's clerks~\cmdsLeg to clear conflicting read capabilities. Meanwhile, it revokes any conflicting write leases it gave out.}
	\label{fig:write}
\end{figure}
%
\subsection*{Namespace Changes}
Most changes to the namespace need simple edits within ministries metadata table. The client sends its request to the minister. The change is performed by adding a command to the \textit{Group Raft} log (see: \Cref{sec:praft}). Before committing the change the client gets the log index for the change. If the minister goes down before acknowledging success the client verifies if the change happened using the log index.

Removing a directory spanning one or more load balanced subtrees needs a little more care. One or more ministers will have to delete their entire subtree. This requires coordination across the entire cluster. The clients remove requests is forwarded by the minster to the \textit{president}. It in turn appends \textsl{Subtree Delete} to the cluster wide log. The client receives the log index for the command to verify success even if the \textit{president} goes down. The steps the minister takes are shown in \Cref{fig:rm}. 
%
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/del_dir.tex}
	\caption{A minister~\amdsLeg{}, here $\text{Minister}_j$, removes a directory (tree) that is load balanced between multiple ministries. The president~\presidentLeg{} coordinates the removal by appending a command to the log. Once it is committed the ministers hosting subtrees of the directory demote themselves to idle and $\text{Ministry}_j$ drops the directory from its db}
	\label{fig:rm}
\end{figure}
%
\subsection{Availability}
Ensuring file system availability is the responsibility of the \textit{president}. This includes replacing nodes that fail and load balancing. All ministers and clerks periodically send heartbeats to the president. In this design I improve scalability by moving tasks from a central authority (the president) to groups of nodes. Continuing this pattern we could let the ministers monitor their clerks. This is more complex than letting these report directly to the \textit{president}. However, even reporting directly to the president is not needed, instead I use the TCP ACK to the \textit{president}'s \raft{} heartbeat. When the \textit{president} fails to send a \raft{} heartbeat to a node it decides the node must be failing.

\subsubsection*{A failing minister}
When the president notices a minister has failed it will try to replace it. It queries the ministries clerks to find the ideal candidate for promotion. If it gets a response from less than half the ministry it can not safely replace the minister. At that point it marks the file subtree as down and may retry in the future. A successful replacement is illustrated in \Cref{fig:appoint}.

A clerk going down is handled by the president in one of two ways:
\begin{itemize}
	\item There is at least one idle node. The president assigns the idle node to the failing nodes group. 
	\item There are no idle nodes. The president, through raft, commands a clerk in the group with the lowest load to demote itself. Then the president drops the matter. The clerk wait till its read leases expire and unassigns.
\end{itemize}
%
When the groups' minister appends to the \textit{Group Raft} log it will notice the clerk misses entries and update it (see \cref{sec:raft}).

\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/apoint.tex}
	\caption{A minister~\amdsLeg{} fails and does not send a heartbeat on time (1). The president~\presidentLeg{} requests the latest commit index (2). Node $\text{Clerk}_1$~\cmdsLeg{} has commit index $k$ which is the highest (3). The president has promoted $\text{Clerk}_1$ to minister, it has started sending heartbeats (4). Note the heartbeats send by the clerks are not shown.}
	\label{fig:appoint}
\end{figure}
%
\subsubsection*{Load balancing} \label{sec:loadb}
From the point of availability a system that is up but drowning in requests might as well be down. To prevent nodes from getting overloaded we actively balance the load between ministries, add and remove them and expand the read capacity of some. A load report contains CPU utilization for each node in the group and the popularity of each bit of metadata split into read and write. The President checks the balance for each report it receives.
%
\subparagraph*{Trade off} \label{sec:tradeoff}
There is a trade-off here: a larger ministry means more clerks for the minister to communicate changes to, slowing down writes. On the other side as the file system is split into more subtrees the chance increases a client will need to contact not one but multiple ministries. Furthermore, to create another ministry we usually have to shrink existing ministries. Growing a ministry can involve removing one to free up clerks. 
%
We can model the read and write capacity of a single group as:
\begin{align}
	r =& n_\text{c} \\
	w =& 1 - \sigma*n_\text{c}
\end{align}%
Here $n_\text{c}$ is the number of clerks in the group, and $\sigma$ the communication overhead added by an extra clerk. 

Now we can derive the number of clerks needed given a load. Since we want to have some unused capacity $\delta$. I set $\delta$ equal to the capacity with the current load subtracted. This gets us the following system of equations: 
\begin{align}
	\delta &= w - W \\
	\delta &= r - R
\end{align}%
Now solve for $n_\text{c}$, the number of clerks.
\begin{align}
	r - R &= w - W \\
	n_c - R &= \left(1 - \sigma*n_c\right) - W \\
	n_c &= \frac{R - W + 1}{1 + \sigma}
\end{align}
From this I draw two conclusions, any spare capacity for writing is at a cost of spare reading capacity. The number of clerks roughly the read load.
%
% TODO a load balancing algorithm
%
\subparagraph*{Read balancing}
A ministry experiencing low read load relative to their capacity is shrunk. A ministries read load is low if it could lose a clerk without average clerk CPU utilization rising above 85\%. A ministry has multiple members not only for performance. The members form a \textit{Group Raft} cluster and ensure metadata is stored redundantly. This only works if a ministry has at least three members. To shrink a ministry the President issues a \raft{} message unassigning a clerk. 

A group under high relative read load has average clerk CPU utilization passing 95\%. The president then issues a \raft{} message assigning an idle node to the group if one is available. %TODO diff % why
%
\subparagraph*{Write balancing}
When the president decides a group can no longer handle the write-load it will try to split off some work to another group. If no existing group can handle the work the president will try to create a new group. If that is not possible then the 



% TODO subdiagram unassign node

\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/subtree.tex}
	\caption{A minister~\amdsLeg{} under too high a load, higher than all other, sends a load report to the president~\presidentLeg{}. It can not create a new ministry as there are no or not enough idle nodes~\umdsLeg{}. The president removes three clerks~\cmdsLeg{} from the ministries under the lightest read load and queues the load report. After the clerk removal is committed the load report is enqueued. }
	\label{fig:subtree}
\end{figure}
