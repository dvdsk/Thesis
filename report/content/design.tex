Here I will go over \name{}'s design and how it enables scalable consistent distributed file storage. First I will discuss the \ac{api} exposed by my system then I will present the architecture, finally I will detail some of the system behaviour using state machine diagrams. 
%
\subsection{API and Capabilities}
\Name{} is a hierarchical file system: data is stored in files which are stored in folders. Folders can contain other folders. The file system hierarchy looks like a tree where each level may have 1 to n nodes.

The \ac{posix} has allowed applications to be developed once for all complying filesystem and forms the basis of \name{}'s \ac{api}. Expanding beyond \ac{posix}, realising performance gains in the new methods, excludes existing applications from these gains. \ceph{}~\cref{sec:ceph} does this trading some usability for performance by implementing part of the \ac{posix} \ac{hpc} IO extensions~\cite{hpc_posix}. \ceph{} also trades in all consistency for files using the \ac{hpc} \ac{api}. 

A key to my approach is expanding the file system \ac{api} beyond posix. While this makes my system harder to use it gains back the consistency lost by \ceph{} while keeping its scalability. Where \name{} expands upon \ac{posix} is the addition of \textsl{open region}. A client that \textit{opens} a file \textit{region} can access to only a range of data in the file. This \ac{api} enables consistent limited \emph{parallel concurrent} writes on or combinations of reading and writing in the same file.

Like in \ceph{} in \name{} clients gain capability when they open files. These capabilities determine what a client may perform on a file. There are four capablities: \nopagebreak
%
\begin{multicols}{4} % 2 does not work, anyway this way the list is nice, tight and centerd
\begin{itemize}
	\item read
	\item read region
	\item write
	\item write region
\end{itemize}
\end{multicols}
%
A client with write capabilities may also read data. 

\begin{samepage}
An \ac{mds} tracks the capabilities for a each of its files. The \ac{mds} will not give out a capability that breaks the following condidtion:
%
\begin{itemize}
	\item Multiple clients can have capabilities on the same file as long as write capabilities have no overlap with any region.
\end{itemize}
\end{samepage}
% 
\subsection{Architecture}
\begin{figure}
	\input{figs/diagrams/arch.tex}
	\caption{An overview of the architecture. Note there are $n$ \acf{amds} groups, each group can have a different number of \acfp{cmds}. Not all servers in a cluster are always actively used as represented by the grey \acfp{umds}}
\end{figure}

\Name{} uses hierarchical leadership, there is one president elected by all \acp{mds}. The president in turn appoints multiple \acfp{amds} then assigns each a group of \acfp{umds}. An \ac{amds} contacts its group, and promotes each member from unassigned to caching. 

The president coordinates the cluster: monitoring the population, assigning new \acp{amds} on failures, adjusting group given failures and load balances between all groups. Load balancing is done through subtree partitioning~\cite{ceph} (see:~\cref{sec:subtree}) with each subtree assigned to a group. To enable the president to make load balancing decisions each \ac{amds} periodically sends file innode popularity.

Metadata changes are coordinated by an \ac{amds} it serializes metadata modifying requests and ensures the changes proliferate to the group's \acp{cmds}. Changes are only completed when they are written to half of the \acp{cmds}. Additionally the \ac{amds} keeps track of file innode popularity querying its \acp{cmds} periodically.

A groups \acp{cmds} handle metadata queries: issueing read capabilities and providing information about the filesystem tree. Additionally each \ac{cmds} tracks file innode popularity to provide to the \ac{amds}. 
%
\subsubsection*{Consensus}
Consensus communication has a performance penalty and complicates system design. Not all communication is critical to data integrity or system. I use simpler protocols where possible. 

The president is elected using \raft{}. Its coordination is communicated using \raft{}'s log replication. On failure a new president is elected by all \acp{mds} using \raft{}.

Communication from the \ac{amds} to its group members uses log replication similar to \raft{}. When an \ac{amds} is appointed it gets a \raft{} term. Changes to the metadata are shared with \acp{cmds} using log replication. When the \ac{amds} fails the president selects the \ac{cmds} with the highest \textsl{commit index} as the new \ac{amds}.

Load reports to the president are send over normal TCP ensuring the arrive in order. An \ac{amds} that froze, was replaced and starts working again however can still send outdated reports. By including the term of the sending \ac{amds} the president detects outdated reports and discards them.
%
\subsection{Operations}
The workings of \name{} can be split in two catagories: handling client requests and the presidents coordinating of the \ac{mds} groups. Lets go over two operations for each of these catagories and show how these are fault tolerant.

\subsubsection*{Client requests}
% - get ranged write capability
% - delete directory spread over two groups

\subsubsection*{Coordination}
% - elect a new president after failure
% - split off a subtree from a group and move it over
