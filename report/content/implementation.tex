Here we will go over the implementation of the design, which is implemented in \textit{Rust}. I begin by motivating the choice for \textit{Rust}. Following that I will go over the concurrency model. Then I will use small extracts of source code to discuss the structure of the implementation. Next we take a more detailed look at my implementation of the Raft (see: \cref{sec:raft}) and discuss why I could not use existing implementations. Finally, we will see how the file leases are implemented.

\subsection{Language}
Distributed systems are notoriously hard to implement with many opportunities for subtle bugs to slip in. Therefore, it is important to choose a language with features that aid our implementation and make it harder to introduce bugs. Let's look at a feature that can help us and one that could become problematic.

A strongly typed language with algebraic data types makes it possible to express properties of the design in the type system. An example: \textit{Clerks} are listing for messages from the \textit{President} or \textit{Minister}, we keep these separate by listening on different ports. Normally a port is expressed as a number. If we make the President's port a different type then the Ministers the type checker will never allow us to switch these around. This is known as \ac{tdd}.

Timing is critical in this design, if the president does not send heartbeats in time elections will pop up. Languages using \ac{gc} pause program execution once every while to clean up memory. This can cause timing problems, also known as the \textit{stop the world problem}. It is possible but hard to mitigate this by carefully tweaking the \ac{gc} to keep its pauses long. If possible we should use a language without \ac{gc}.

Only the \textit{Rust} language has such a type system without using \ac{gc}. The language guarantees an absence of data races which makes a concurrent implementation far easier.

\subsection{Concurrency}
While sending and receiving data over a network most time is spent waiting. Blocking the implementation while waiting is not at all efficient. Instead, we can use the valuable time to start and or finish sending and receiving other data concurrently. Usually this is solved by spawning a thread for each connection. Another way of doing this is using \textit{non-blocking IO}, however organizing a single thread of execution to use of non-blocking-IO becomes highly complex at scale. 
Maintaining file leases requires us to hold many concurrent connections~\cref{sec:lease}. Using one thread for each connection could limit the number of connections, therefore we can only rely on Non-blocking IO. To get around the problematic complexity we use: \textit{Async/await}\footnote{See \cref{app:async} for an introduction to Async/await}. Async/await is a language feature which allows us to construct and combine non-blocking functions as if they were normal functions. \textit{Rust} has native support for the needed syntax however requires a third party framework to provide the actual IO implementation, here I use the \textit{Tokio} project~\cite{tokio}.

There is a trend in distributed systems to take scalability as the holy grail of performance~\cite{scaling}. While the design of the system focuses on scalability in my implementing I try to optimally use the underlying hardware. While \textit{Moors~Law} might by dying single machine performance will keep scaling horizontally~\cite{moore}. This means the implementation must take full advantage of the available task parallelism. Fortunately the above-mentioned framework \textit{Tokio} provides tasks which combine organized non-blocking-IO with parallel execution. All tasks are divided into groups with each group running on a single OS-thread. Creating and destroying tasks is fast compared to OS threads.

Concurrency is mostly achieved by passing messages between tasks. Where needed these messages include a method to communicate back completion. There is also some shared state to keep track of the Raft lock, however it is contained to the \textsl{raft} module. By using message passing less time is spent waiting on locks and deadlocking bugs are contained to sections using shared state. 

\subsubsection*{Cancelling tasks}
In \name's design we frequently need to abort a concurrently running task. Clerks for example handle client requests in a concurrently running task. When a clerk becomes president it needs to stop handling those requests. If we were using threads we would do this by changing a shared variable from the outside. The task would be written such that it frequently checks if the variable is changed and when it is the task returns. 

Whenever an \textit{async} function has to await IO it returns control to the scheduler. When IO is ready the scheduler can choose to continue the function. We can ask it not to and instead cancel the task. As Rust enforces \acf{raii}~\cite[p.~389]{raii}~\footnote{A programming idiom where acquiring a resource is done when creating an object. When the object is destroyed code runs that release or cleans up the object} the framework must drop all the objects in the scope of canceled tasks. 
Task handles instruct the framework to cancel their task when they are dropped. A group of tasks can be canceled by dropping the data structure that contains the task handles. By organizing concurrent tasks as a tree with the root the \textsl{main function} cancelling and cleaning up a branch is as easy as dropping the task handle for the root of that branch. Concretely if we abort the \textsl{president} task we automatically end any tasks it created.

\subsection{Structure}
Nodes in \name{} switch between the role of \textit{president}, \textit{minister}, \textit{clerk} and \textit{idle}. The roles are separate functions. When a node switches role it returns from one function and enters the one corresponding with its new role. The switching is implemented in the state machine seen in \cref{lst:state}. In Rust expressions return a value, the \lstinline[language=rust]{match} statement in line 2 returns the \lstinline[language=rust]{role} for the next iteration. The different \lstinline[language=rust]{work} functions set up the async tasks needed before waiting for an exit condition.
%
\begin{lstlisting}[float,language=rust,style=boxed,tabsize=2,caption={The state machine switching between a nodes different roles},label=lst:state]
let mut role = Role::Idle;
loop {
	role = match role {
		Role::Idle => idle::work(&mut state).await.unwrap(),
		Role::Clerk { subtree } => {
			clerk::work(&mut state, subtree).await.unwrap()
		}
		Role::Minister {
			subtree,
			clerks,
			term,
		} => minister::work(&mut state, subtree, clerks, term)
			.await
			.unwrap(),
		Role::President { term } => {
			president::work(&mut state, &mut chart, term).await
		}
	}
}
\end{lstlisting}
\clearpage
%
Before nodes enter the state machine we set up two \raft{} logs. The president log handles messages, timing out on inactivity and holding elections in a background task. The minister log handles only receiving messages. In both cases newly committed log entries (or orders) are made available through a queue to a \raft{} Log object. Election losses and wins are also communicated through the queue.

Let us now take a look at the president work function see~\cref{lst:pres}. We enter it if we are elected president while in one of the other roles.
%
The function takes a number of arguments one of which is the presidential \raft{} Log. It is destructed into its parts: the queue, and the \raft{} state. A LogWriter is created which allows appending to the raft log and waiting till the appended order is committed. Finally, we create the LoadBalancer. The created objects are passed to the async functions or tasks: 
\begin{itemize}
	\item \textsl{load\_balancing}: issues orders using the LogWriter assigning nodes and file subtrees to ministries, re-assigns based on events such as: nodes going down, coming back online, new being added and ministry load.
	\item \textsl{instruct\_subjects}: run the leader part of the \raft{} algorithm. Shares log entries with all other nodes and tracks which can be committed then applying them locally.
	\item \textsl{handle\_incoming}: handle requests, redirecting clients to ministries.
	\item \textsl{recieve\_own\_order}: apply committed orders from the \raft{} Log queue to the programs state.
\end{itemize}
%
These are then selected on, that is run concurrently until one of them finishes. Specifically here they run until \textsl{recieve\_own\_order} ends. This happens when the \raft{} background task inserts a \lstinline[language=rust]{ResignPres} order indicating a higher termed president was noticed. At this point the president work function finishes returning the next role: Idle.

%
\begin{lstlisting}[float,language=rust,style=boxed,tabsize=2,caption={The president work function, it performs all the tasks of the president. In this code snippet brackets and parenthesis containing whitespace mean the corrosponding structs and functions have there arguments hidden for brevity},label=lst:pres]
pub(super) async fn work( ) -> crate::Role {
	let Log { orders, state, .. } = pres_orders;
	let (broadcast, _) = broadcast::channel(16);
	let (tx, notify_rx) = mpsc::channel(16);

	let log_writer = LogWriter { };

	let (load_balancer, load_notifier) = LoadBalancer::new( );
	let instruct_subjects = subjects::instruct( );
	let load_balancing = load_balancer.run( );

	tokio::select! {
		() = load_balancing => unreachable!(),
		() = instruct_subjects => unreachable!(),
		() = msgs::handle_incoming(client_lstnr, log_writer) => {
			unreachable!(),
		}
		res = recieve_own_order(orders, load_notifier) => {
			Role::Idle
		}
	}
}
\end{lstlisting}

The other work functions similarly select on multiple async tasks. These tasks themselves create yet other tasks. This way the program builds up a tree of concurrently working functions. The tree is illustrated in \cref{fig:tree}. Work that scales with system load is divided over a variable amount concurrently running tasks. Each connection to a client for example is run in parallel on a separate task.
%
\clearpage
\thispagestyle{empty}
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/structure.tex}
	\caption{Diagram of all concurrently running functions in a node. A dashed line between items means only of those items will be running at the time. For example a node in the Idle role can not concurrently be a Minister. Functions in red~\taskLeg{} are single tasks while purple~\tasksLeg{} indicates there are between zero and $n$ instances of the function running. Functions in gray~\futureLeg{} are futures, they share a thread with any parent and or child futures}
	\label{fig:tree}
\end{figure}
\clearpage

\subsection{Raft}
There are a lot of reliable \raft{} implementations. Developing my own took a lot of time and makes the system less reliable as my implementation has hardly been tested. \Name{} has two needs that where fulfilled by no existing implementation: 
%
\begin{itemize}
	\item \Name{} uses the \raft{} heartbeat to maintain file system consensus (see:~\cref{sec:praft}). For example a fresh clerk needs to know when it is up-to-date\footnote{That is, the clerk has applied all log committed entries, and the last was committed within a \raft{} heartbeat of it being committed} and can begin serving clients. 
	\item \Name{} needs a special version of \raft{}, one where elections are illegal and leaders (minsters) are assigned by a third party (the president). Multiple of these instances (or ministries) must be able to exist simultaneously. The log must stay consistent and clients should see no new entries of an old leader after being assigned to another leader. 
\end{itemize}
%
To demonstrate that \name{} scales and can be optimized in future work\footnote{see:~\cref{sec:profile,sec:discussion}} the same must hold for the custom implementation. If it does not then the design of \name{} could be relying on an implementation detail that fundamentally limits its performance.
%
\subsubsection*{Perishable log entries}
When a \raft{} message arrives it can cause entries in the log to become committed. At that point they are made available to the system. These could be old entries, long ago committed by other nodes. The message contains the index of the last committed entry or entries which we use to recognize if an entry is old. Newly committed messages can still become outdated if they are applied too slowly. This can happen if the server slows down due to bugs in \name{} or hardware issues. The system detects this by keeping the timestamp the message that made an entry committed together with the entry. This combination, a \textit{perishable entry} is made available to \name{}.

% TODO: Synchronize terminology with design sectin (dictatorial vs praft) 
%       "group raft" might be an option here <15-07-22, dvdsk noreply@davidsk.dev> 
%       change the \graft{} command in main
\subsubsection*{\graft{}} \label{sec:dictraft}
Here a third party instructs a node to become the leader. Followers are not informed directly but rather accept the new leader as it has a higher term. When receiving a message the normal \raft{} rules apply therefore messages from the old leader\footnote{The third party replacing the leader usually indicates there is a problem with the current leader, making it doubly important that they are ignored} will be rejected as there term is too low.

Nodes must be able to move between groups with assigned leaders. This present two problems, first:
\begin{itemize}
	\item Leader B receives message that follower x is assigned to it
	\item Leader B appends to its log and sends an append request to its followers now including x
	\item Follower x \emph{accepts} the request as x has a \emph{lower} term then the request
	\item Follower x \emph{increases} its term to match B
	\item Leader A receives a message that assigns x back to it
	\item Leader A appends to its log and sends an append request to its followers which now again includes x
	\item Follower x \emph{rejects} the request as x now has a \emph{higher} term then the request
\end{itemize}
Leader A must initially have a lower term then B and then a higher term then B. Secondly we need a follower to re-write its log after every move to match that of its new group. 

To solve the first we make the third party change the term of the leader when it assigning it a node. If it guarantees the new term is the highest of all the groups every reassignment will succeed. It is simple to assure this using a single third party incrementing the term every assignment and re-assignment. 

As the highest number is always unique this coincidentally also solves our second problem. The correct log for each group now has an increasing sequence of unique terms. If a node receives an append request and its previous log entries term and index do not match that of the leader it rejects the request. The leader then starts sending older log entries\footnote{This is optimized in \name{} by clearing the entire log if a clerk came from another ministry}. Given terms are now unique to groups a successful append can only happen on correct (partial) group log.

\subsection{File leases}
As discussed in \cref{sec:arch} read and write access is coordinated by a file's ministry. Before issuing write access a minister must ensure outstanding reads leases are revoked. Similarly, clerks must ensure they do not offer read-leases to files that can be written too. The minister \emph{locks} the needed file on all the ministries clerks before issuing a write-lease. 

Managing these read locks is the responsibility of the \textit{lock manager} which runs concurrent to the ministers other tasks (see~\cref{fig:tree}). When the client connection handler \straightTasksLeg{} receives a write request it enters a \rust{write_lease} function. This checks if it has already given out a write-lease, returning an error if it has. Then the \textit{lock manager} is requested to lock the file. A lease-guard is constructed once the file has been locked on the clerks. The guard will unlock the file if the handler leaves the \rust{write_lease} function. Then the client is returned the lease together with a time before which it needs to be renewed. For as long as the client keeps sending \rust{RefreshLease} on time the handler stays in the \rust{write_lease} function.

Leases are not stored to hard drive they are volatile. When a clerk goes down all leases issued by it are lost and clients will need to reacquire them. A minister going down means loss of all the write-leases however the clerks can keep issuing leases as usual. The new minister will unlock all files when it comes online. These rules allow \name{} to use simple TCP messaging instead of relying on \raft{}. Assuming files access more common than file creation and removal optimizing lease management will speed up \name{}.
%
\subsubsection*{Locking Rules}
The \textit{lock manager} times its lock requests to clients to ensure consistency and correctness. It is easiest to explain this at the hand of an example. Here a clerk gets partitioned off from the rest of the cluster at the worst possible time:
%
\begin{itemize}
	\item A minister receives a write request for file F
	\item At time $T$ clerk A receives its last heartbeat from the President
	\item Clerk A loses connection to the rest of the cluster but stays reachable for clients.
	\item The lock manager fans out a lock request for F, it can not reach clerk A and starts retrying.
	\item Just before time $T+H$ clerk A issues a read lease to a client, it is valid until just before $T+2H$
	\item At time $T+H$ clerk A misses the next heartbeat and stops handling client requests
	\item Just before time $T+2H$ the client fails to refresh its lease and stops reading
	\item After $2H$ the lock manager gives up. It is guaranteed that any outstanding read-lease issued by clerk A has now expired
	\item The minister issues the write-lease for F
\end{itemize}
%
We see that $2H$ after the lock manager started trying to lock the file it can assume the file locked. A clerk going offline will increase file access for $2H$. If the manager keeps trying to reach it we keep this $2H$ overhead. Instead, the manager will remove the clerk before handling another request. Without any failures file write access time should be dominated by the latency of the lock TCP roundtrips.
%
\subsubsection*{Performance}
The lock manager has been written to handle many simultaneous requests. It is therefore lockless and holds open TCP connection to its clerks. The minister communicates with the manager through message passing. When clerk gets assigned by the president the lock manager receives a message. It then opens up a connection in a new concurrent task dedicated to this clerk. 

The decisions the lock manager makes directly impact the rest of the cluster. Each lock placed on a clerk potentially blocks read-leases which potentially slows down read performance. Therefore, it is important to unlock as soon as possible. The lock manager thus prioritizes unlock above lock requests.
%
\subsubsection*{Known problems}
The current implementation has four known problems. Three of these have simple solutions however third requires changes to the design. First, an imposter or failing node can still send unlock requests. Including the current minister term in the request and checking if its valid would solve this\footnote{Similar to \graft{}, see~\cref{sec:dictraft}}. 

Second, a newly assigned clerk can serve clients before it has processed all the existing locks. Clerks already get their ministerial \raft{} log up-to-date before they start serving requests. The same should be done for lock requests. 

Third, a network fault could make it impossible for \textit{only} a minister, and thus lock manager, to reach one of its clerks. Traffic from the president and clients would still reach the clerk. In this case the lock manager will assume a file locked after $2H$ while the clerk does not miss a heartbeat and stays up. This clerk could now enable reading to a file that is being written to. We can prevent this by making the minister inform the president off the clerk's failure. The president would then exclude the clerk from heartbeats triggering its shutdown on time. 

Finally, a lock request can fail when the file has not yet been created on a clerk. In \raft{} a log entry becomes committed after the majority has accepted it. In the current implementation file creation is done as soon as the corresponding log entry is committed. A clerk that is behind in processing log entries can receive and start processing lock requests. Unfortunately this reveals a design problem: \textit{there is no mechanism to handle a clerk lagging behind}. In section \cref{sec:fut} we will discuss how the design can change to address this.
