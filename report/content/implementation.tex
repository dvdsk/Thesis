Here we will go over the implementation of the design, which is implemented in \textit{Rust}. I begin by motivating the choice for \textit{Rust}. Following that I will go over the concurrency model. Then I will use small extracts of source code to discuss the structure of the implementation. Next we take a more detailed look at my implementation of the Raft (see: \cref{sec:raft}) and discuss why I did not use existing implementations.

\subsection{Language}
Distributed systems are notoriously hard to implement with many opportunities for subtle bugs to slip in. Therefore, it is important to choose a language with features that aid our implementation and make it harder to introduce bugs. Let's look at a feature that can help us and one that could become problematic.

A strongly typed language with algebraic data types makes it possible to express properties of the design in the type system. An example: \textit{Clerks} are listing for messages from the \textit{President} or \textit{Minister}, we keep these separate by listening on different ports. Normally a port is expressed as a number. If we make the President's port a different type then the Ministers the type checker will never allow us to switch these around. This is known as \ac{tdd}.

Timing is critical in this design, if the president does not send heartbeats in time elections will pop up. Languages using \ac{gc} pause program execution once every while to clean up memory. This can cause timing problems, also known as the \textit{stop the world problem}. It is possible but hard to mitigate this by carefully tweaking the \ac{gc} to keep its pauses long. If possible we should use a language without \ac{gc}.

Only the \textit{Rust} language has such a type system without using \ac{gc}. The language guarantees an absence of data races which makes a concurrent implementation far easier.

\subsection{Concurrency}
While sending and receiving data over a network most time is spent waiting. Blocking the implementation while waiting is not at all efficient. Instead, we can use the valuable time to start and or finish sending and receiving other data concurrently. Usually this is solved by spawning a thread for each connection. Another way of doing this is using \textit{non-blocking IO}, however organizing a single thread of execution to use of non-blocking-IO becomes highly complex at scale. 
Maintaining file leases requires us to hold many concurrent connections~\cref{sec:lease}. Using one thread for each connection could limit the number of connections, therefore we can only rely on Non-blocking IO. To get around the problamitic complexity we use: \textit{Async/await}\footnote{see \cref{app:async} for a introduction to Async/await}. Async/await is a language feature which allows us to construct and combine non-blocking functions as if they were normal functions. \textit{Rust} has native support for the needed syntax however requires a third party framework to provide the actual IO implementation, here I use the \textit{Tokio} project~\cite{tokio}.

There is a trend in distributed systems to take scalability as the holy grail of performance~\cite{scaling}. While the design of the system focuses on scalability in my implementing I try to optimally use the underlying hardware. While \textit{Moors~Law} might by dying single machine performance will keep scaling horizontally~\cite{moore}. This means the implementation must take full advantage of the available task parallelism. Fortunately the above-mentioned framework \textit{Tokio} provides tasks which combine organized non-blocking-IO with parallel execution. All tasks are divided into groups with each group running on a single OS-thread. Creating and destroying tasks is fast compared to OS threads.

Concurrency is mostly achieved by passing messages between tasks. Where needed these messages include a method to communicate back completion. There is also some shared state to keep track of the Raft lock, however it is contained to the \textsl{raft} module. By using message passing less time is spent waiting on locks and deadlocking bugs are contained to sections using shared state. 

\subsubsection*{Cancelling tasks}
In \name's design we frequently need to abort a concurrently running task. Clerks for example handle client requests in a concurrently running task. When a clerk becomes president it needs to stop handling those requests. If we were using threads we would do this by changing a shared variable from the outside. The task would be written such that it frequently checks if the variable is changed and when it is the task returns. 

Whenever an \textit{async} function has to await IO it returns control to the scheduler. When IO is ready the scheduler can choose to continue the function. We can ask it not to and instead cancel the task. As Rust enforces \acf{raii}~\cite[p.~389]{raii}~\footnote{A programming idiom where acquiring a resource is done when creating an object. When the object is destroyed code runs that release or cleans up the object"} the framework must drop all the objects in the scope of canceled tasks. 
Task handles instruct the framework to cancel their task when they are dropped. A group of tasks can be canceled by dropping the data structure that contains the task handles. By organizing concurrent tasks as a tree with the root the \textsl{main function} cancelling and cleaning up a branch is as easy as dropping the task handle for the root of that branch. Concretely if we abort the \textsl{president} task we automatically end any tasks it created.

\subsection{Structure}
Nodes in \name{} switch between the role of \textit{president}, \textit{minister}, \textit{clerk} and \textit{idle}. The roles are separate functions. When a node switches role it returns from one function and enters the one corresponding with its new role. The switching is implemented in the state machine seen in \cref{lst:state}. In Rust expressions return a value, the \lstinline[language=rust]{match} statement in line 2 returns the \lstinline[language=rust]{role} for the next iteration. The different \lstinline[language=rust]{work} functions set up the async tasks needed before waiting for an exit condition.
%
\begin{lstlisting}[float,language=rust,style=boxed,tabsize=2,caption={The state machine switching between a nodes different roles},label=lst:state]
let mut role = Role::Idle;
loop {
	role = match role {
		Role::Idle => idle::work(&mut state).await.unwrap(),
		Role::Clerk { subtree } => {
			clerk::work(&mut state, subtree).await.unwrap()
		}
		Role::Minister {
			subtree,
			clerks,
			term,
		} => minister::work(&mut state, subtree, clerks, term)
			.await
			.unwrap(),
		Role::President { term } => {
			president::work(&mut state, &mut chart, term).await
		}
	}
}
\end{lstlisting}
\clearpage
%
Before nodes enter the state machine we set up two \raft{} logs. The president log handles messages, timing out on inactivity and holding elections in a background task. The minister log handles only receiving messages. In both cases newly committed log entries (or orders) are made available through a queue to a \raft{} Log object. Election losses and wins are also communicated through the queue.

Let us now take a look at the president work function see~\cref{lst:pres}. We enter it if we are elected president while in one of the other roles.
%
The function takes a number of arguments one of which is the presidential \raft{} Log. It is destructed into its parts: the queue, and the \raft{} state. A LogWriter is created which allows appending to the raft log and waiting till the appended order is committed. Finally, we create the LoadBalancer. The created objects are passed to the async functions or tasks: 
\begin{itemize}
	\item \textsl{load\_balancing}: issues orders using the LogWriter assigning nodes and file subtrees to ministries, re-assigns based on events such as: nodes going down, coming back online, new being added and ministry load.
	\item \textsl{instruct\_subjects}: run the leader part of the \raft{} algorithm. Shares log entries with all other nodes and tracks which can be committed then applying them locally.
	\item \textsl{handle\_incoming}: handle requests, redirecting clients to ministries.
	\item \textsl{recieve\_own\_order}: apply committed orders from the \raft{} Log queue to the programs state.
\end{itemize}
%
These are then selected on, that is run concurrently until one of them finishes. Specifically here they run until \textsl{recieve\_own\_order} ends. This happens when the \raft{} background task inserts a \lstinline[language=rust]{ResignPres} order indicating a higher termed president was noticed. At this point the president work function finishes returning the next role: Idle.

%
\begin{lstlisting}[float,language=rust,style=boxed,tabsize=2,caption={The president work function, it performs all the tasks of the president. In this code snippet brackets and parenthesis containing whitespace mean the corrosponding structs and functions have there arguments hidden for brevity},label=lst:pres]
pub(super) async fn work( ) -> crate::Role {
	let Log { orders, state, .. } = pres_orders;
	let (broadcast, _) = broadcast::channel(16);
	let (tx, notify_rx) = mpsc::channel(16);

	let log_writer = LogWriter { };

	let (load_balancer, load_notifier) = LoadBalancer::new( );
	let instruct_subjects = subjects::instruct( );
	let load_balancing = load_balancer.run( );

	tokio::select! {
		() = load_balancing => unreachable!(),
		() = instruct_subjects => unreachable!(),
		() = msgs::handle_incoming(client_lstnr, log_writer) => {
			unreachable!(),
		}
		res = recieve_own_order(orders, load_notifier) => {
			Role::Idle
		}
	}
}
\end{lstlisting}

The other work functions similarly select on multiple async tasks. The tasks themselves often create yet other tasks. This way the program builds up a tree of concurrently working functions. The tree is illustrated in \cref{fig:tree}.

\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/structure.tex}
	\caption{Structure of the program visualized as a tree of all concurrently running tasks}
	\label{fig:tree}
\end{figure}
% (mathy) conditions from 
% note all are ranged just range > file for unranged
% mocking
% async
% find amds: 
% 	- use sorted loadbalance db
% 	- find suffix that matches end part of target path
% subset of \ac{posix} that is implemented
%
% use rust borrow to enforce correctness (ElectionDataGuard)
