Here we go over the implementation of the design, which is written in \textit{Rust}. I begin by motivating the choice for \textit{Rust}. Following that I go over the concurrency model. Then using small extracts of source code I discuss the structure. Next we take a more detailed look at my take on Raft (see: \cref{sec:raft}) and discuss why I not use existing libraries. Finally, we see how the file leases are implemented.

\subsection{Language}
Distributed systems are notoriously hard to build with many opportunities for subtle bugs to slip in. Therefore, it is important to choose a language with features that aid our work and make it harder to introduce bugs. Let's discuss a feature that can help us and one that could become problematic.

A strongly typed language with algebraic data types enables us to express properties of the design in the type system. An example: \textit{Clerks} are listing for messages from the \textit{President} or their \textit{Minister}, we keep these separate by listening on different ports. Normally a port is expressed as an integer. If we make the type of the President's port different from the Ministers the compiler will prevent us from switching these around. This is known as \ac{tdd}.

Timing is critical for the design, if the president does not send heartbeats in time elections might start. Languages using \ac{gc} pause program execution once every while to clean up memory. This can cause timing problems, also known as the \textit{stop the world problem}. It is possible but hard to mitigate this by carefully tweaking the \ac{gc} to keep its pauses short. If possible we should use a language without \ac{gc}.

Only the \textit{Rust} language has such a type system without using \ac{gc}. Furthermore, the language guarantees an absence of data races which makes a concurrent implementation far easier.

\subsection{Concurrency}
When sending and receiving data over the network most time is spent waiting. Blocking while waiting is not at all efficient. We can use the valuable time to start and or finish sending and receiving other data concurrently. Usually this is solved by spawning a thread for each connection. Another way of doing this is using \textit{non-blocking IO}, however organizing a single thread of execution to use non-blocking-IO for a diverse set of concurrent operations becomes highly complex. 
Maintaining file leases~(see:~\cref{sec:lease}) requires us to hold many concurrent connections. On the other hand one thread for each connection could limit the number of connections as we run out of threads. To get around the problematic complexity of non-blocking-IO we use: \textit{Async/await}\footnote{See \cref{app:async} for an introduction to Async/await}. It is a language feature which allows us to construct and combine non-blocking functions as if they were normal functions. \textit{Rust} has native support for the needed syntax but requires a third party framework to provide the actual IO implementation, here I use the \textit{Tokio} project~\cite{tokio}.

There is a trend in distributed systems to take scalability as the holy grail of performance~\cite{scaling}. While the design of the system focuses on scalability my implementation tries to use the underlying hardware optimally. 
While \textit{Moor'se~Law} might not apply any more single machine performance will keep scaling horizontally~\cite{moore}. In recent years we see this scaling in the form of increasing core counts\footnote{Enabled by CPU chiplets: multiple smaller dies that are combined into a single multicore CPU.}. This means the implementation should proof the design future-proof by taking full advantage of available task parallelism. Fortunately the above-mentioned framework \textit{Tokio} provides tasks which combine organized non-blocking-IO with parallel execution. These tasks are divided into groups where each group runs concurrently on a single OS-thread. Creating and destroying tasks is fast compared to OS threads.

Sharing state concurrently is with few exceptions achieved by passing messages between tasks. Where needed these include a method to signal back completion. Some shared state is used to keep track of the Raft lock, it is contained to the \textsl{raft} module. By mostly using message passing less time is spent waiting on locks and deadlocking bugs are contained to sections using shared state\footnote{Most of the message passing does not block, instead has a small buffer and returns an error if the buffer is full.} 

\subsubsection*{Cancelling tasks}
In \name's design we frequently need to abort a concurrently running task. Clerks for example handle client requests in a concurrently running task. When a clerk becomes president it needs to stop handling those requests. If we were using threads we would do this by changing a shared variable. The task would be written such that it frequently checks if the variable is changed and when it is the task returns. 

Whenever an \textit{async} function has to await IO it returns control to the scheduler. When IO is ready the scheduler can choose to continue the function. We can ask it not to effectively cancelling the task. Since Rust enforces \acf{raii}~\cite[p.~389]{raii}~\footnote{A programming idiom where acquiring a resource is done when creating an object. When the object is destroyed code runs that release or cleans up the object} the framework drops all the objects in the scope of canceled tasks. 

Task handles instruct the framework to cancel their task when they are dropped. A group of tasks can be canceled by dropping the data structure that contains their task handles. We organize concurrent tasks as a tree, cancelling and cleaning up an entire branch is as easy as dropping the task handle for the root of that branch. Concretely if we abort the \textsl{president} task we automatically end any tasks it created.

\subsection{Structure}
Nodes in \name{} switch between the role of \textit{president}, \textit{minister}, \textit{clerk} and \textit{idle}. The roles are separate functions. When a node switches role it returns and enters the function corresponding with its new role. The switching is implemented in the state machine seen in \cref{lst:state}. In Rust expressions return a value, the \lstinline[language=rust]{match} statement in line 2 returns the \lstinline[language=rust]{role} for the next iteration. The different \lstinline[language=rust]{work} functions set up the async tasks needed, then they start waiting for an exit condition.
%
\begin{lstlisting}[float,language=rust,style=boxed,tabsize=2,caption={The state machine switching between a nodes different roles},label=lst:state]
let mut role = Role::Idle;
loop {
	role = match role {
		Role::Idle => idle::work(&mut state).await.unwrap(),
		Role::Clerk { subtree } => {
			clerk::work(&mut state, subtree).await.unwrap()
		}
		Role::Minister {
			subtree,
			clerks,
			term,
		} => minister::work(&mut state, subtree, clerks, term)
			.await
			.unwrap(),
		Role::President { term } => {
			president::work(&mut state, &mut chart, term).await
		}
	}
}
\end{lstlisting}
\clearpage
%
Before nodes enter the state machine they set up two \raft{} logs. The president log handles messages, timing out on inactivity and holding elections in a background task. The minister log handles only receiving messages. In both cases newly committed log entries are made available through a queue to a \raft{} Log object. Election losses and wins are also communicated through this queue.

Let us take a look at the president work function in~\cref{lst:pres}. We enter it if we are elected president. One of the arguments this function receives is the presidential \raft{} Log. It borrows the logs parts: the queue, and the \raft{} state. The state is wrapped in a \rust{LogWriter} which allows appending to the raft log and waiting till the entry is committed. Finally, a \rust{LoadBalancer} instance is set up. The created objects are passed to the async functions or task, which are: 
\begin{itemize}
	\item \textsl{load\_balancing}: issues orders assigning nodes and file subtrees to ministries using the \rust{LogWriter}, re-assigns based on events such as: nodes going down, coming back online, new being added and ministry load.
	\item \textsl{instruct\_subjects}: performs the leader part of the \raft{} algorithm. Shares log entries with all other nodes and tracks which can be committed.
	\item \textsl{handle\_incoming}: handle requests, redirecting clients to ministries.
	\item \textsl{recieve\_own\_order}: apply committed orders from the \raft{} Log queue to the programs state.
\end{itemize}
%
These tasks are selected on, making them run concurrently until one of them finishes. Here this means they run until \textsl{recieve\_own\_order} returns. This happens when the \raft{} background task inserts a \rust{ResignPres} order indicating a higher termed president was noticed. After the select the president work function finishes returning the next role: Idle.

%
\begin{lstlisting}[float,language=rust,style=boxed,tabsize=2,caption={The president work function, it performs all the tasks of the president. In this code snippet brackets and parenthesis containing whitespace mean the corrosponding structs and functions have there arguments hidden for brevity},label=lst:pres]
pub(super) async fn work( ) -> crate::Role {
	let Log { orders, state, .. } = pres_orders;
	let (broadcast, _) = broadcast::channel(16);
	let (tx, notify_rx) = mpsc::channel(16);

	let log_writer = LogWriter { };

	let (load_balancer, load_notifier) = LoadBalancer::new( );
	let instruct_subjects = subjects::instruct( );
	let load_balancing = load_balancer.run( );

	tokio::select! {
		() = load_balancing => unreachable!(),
		() = instruct_subjects => unreachable!(),
		() = msgs::handle_incoming(client_lstnr, log_writer) => {
			unreachable!(),
		}
		res = recieve_own_order(orders, load_notifier) => {
			Role::Idle
		}
	}
}
\end{lstlisting}

The other work functions similarly select on multiple async tasks. These tasks themselves create yet other tasks. This way the program builds up a tree of concurrently running functions. The tree is illustrated in \cref{fig:tree}. Work that scales with system load is divided over a variable amount concurrently running tasks. Each connection to a client for example is run in parallel on a separate task.
%
\clearpage
\thispagestyle{empty}
\begin{figure}[htbp]
	\centering
	\input{figs/diagrams/structure.tex}
	\caption{Diagram of all concurrently running functions in a node. A dashed line between items means only of those items can be running at the time. For example a node in the Idle role can not concurrently be a Minister. Functions in red~\taskLeg{} are single tasks while purple~\tasksLeg{} indicates there are between zero and $n$ instances of the function running. Functions in gray~\futureLeg{} are futures: they are running concurrently, however share a thread with any parent and or child futures.}
	\label{fig:tree}
\end{figure}
\clearpage

\subsection{Raft}
There are a lot of reliable \raft{} implementations. Developing my own took a lot of time and as my implementation has hardly been tested it makes the system less reliable. Building on existing work was no option as \name{} has two unique requirements: 
%
\begin{itemize}
	\item \Name{} uses the \raft{} heartbeat to maintain file system consensus (see:~\cref{sec:praft}). It is used for example newly assigned clerks need to know when they are up-to-date\footnote{That is, the clerk has applied all log committed entries, and the last was committed within a \raft{} heartbeat of it being committed} and can begin serving clients. 
	\item \Name{} needs a special version of \raft{}, one where elections are rigged and leaders (minsters) are assigned by a third party (the president). Multiple of these instances (or ministries) must be able to exist simultaneously. The log must stay consistent and clients should see no entries of an old leader after being assigned to another leader. 
\end{itemize}
%
To demonstrate that \name{} scales and can be optimized in future work %
%\footnote{see:~\cref{sec:profile,sec:discussion}} 
the custom implementation must also scale and be optimizable. If it does not then the design of \name{} could be relying on an implementation detail that fundamentally limits its performance.
%
\subsubsection*{Perishable log entries}
When a \raft{} message arrives it can cause entries in the log to become committed. At that point they are made available to the system. These could be old entries, long ago committed by other nodes. The message contains the index of the last committed entry or entries which we use to recognize if an entry is old. Newly committed messages can still become outdated if they are applied too slowly. This can happen if the server slows down due to bugs in \name{} or hardware issues. The system notes the time a newly committed entry arrived. The time is combined with the entry into a perishable entry. It is what is made available to \name{} and can be asked whether it is fresh.

\subsubsection*{\graft{}} \label{sec:dictraft}
In group raft a third party instructs a node to become the leader. Followers are not informed directly but rather accept the new leader as it has a higher term. When receiving a message the normal \raft{} rules apply therefore messages from the old leader\footnote{The third party replacing the leader usually indicates there is a problem with the current leader, making it doubly important that messages from old leaders are ignored} are rejected as there term is too low.

Nodes must be able to move between groups with assigned leaders. This present two problems, the first issue becomes apparent looking at the following series of events:
\begin{itemize}
	\item Leader B receives message that follower x is assigned to it
	\item Leader B appends to its log and sends an append request to its followers now including x
	\item Follower x \emph{accepts} the request as x has a \emph{lower} term then the request
	\item Follower x \emph{increases} its term to match B
	\item Leader A receives a message that assigns x back to it
	\item Leader A appends to its log and sends an append request to its followers which now again includes x
	\item Follower x \emph{rejects} the request as x now has a \emph{higher} term then the request
\end{itemize}
Leader A must initially have a lower term then B and then a higher term then B for the re-assignment to work. The second problem: we need a follower to re-write its log after every move to match that of its new group. 

To solve the first problem we make the third party change the term of the leader when it assigning it a node. Every re-assignment will succeed if it guarantees the new term is the highest of all the groups. It is simple to ensure that using a single third party as we can then atomically increment the term every assignment and re-assignment. 

This coincidentally also solves our second problem. Since the highest number is always unique the correct log for each group now has an increasing sequence of unique terms. If a node receives an append request and its previous log entries term and index do not match that of the leader it rejects the request. The leader then starts sending older log entries\footnote{This is optimized in \name{} by clearing the entire log if a clerk came from another ministry}. A successful append can only happen on correct (partial) group log given terms are now unique to groups.

\subsection{File leases} \label{sec:impl_leases}
As discussed in \cref{sec:arch} read and write access is coordinated by a ministry. Before issuing write access a minister must ensure outstanding reads leases are revoked. Similarly, clerks must ensure they do not offer read-leases to files that can be written too. The minister \emph{locks} the needed file on all the ministries clerks before issuing a write-lease. 

Managing these read locks is the responsibility of the \textit{lock manager} which runs concurrent to the ministers other tasks (see~\cref{fig:tree}). When the client connection handler \straightTasksLeg{} receives a write request it enters a \rust{write_lease} function. This checks if it has already given out a write-lease, returning an error if it has. Then the \textit{lock manager} is requested to lock the file. A lease-guard is constructed once the file has been locked on the clerks. The guard unlocks the file if the handler leaves the \rust{write_lease} function. This guarantees the file is unlocked even if the function is aborted due to an error. Then the client is returned the lease together with a time before which it needs to be renewed. For as long as the client keeps sending \rust{RefreshLease} on time the handler stays in the \rust{write_lease} function.

Leases are not flushed to stable storage (hard drives) they are volatile. When a clerk goes down all leases issued by it are lost and clients need to reacquire them. A minister going down means loss of all the write-leases the clerks however can keep issuing leases. The new minister unlocks all files when it comes online. These rules allow \name{} to use simple TCP messaging instead of relying on \raft{} for everything. Assuming files access is more common than file creation and removal optimizing lease management will speed up \name{} significantly.
%
\subsubsection*{Locking Rules}
The \textit{lock manager} times its lock requests to ensure consistency and correctness. It is easiest to explain this at the hand of an example. Here a clerk gets partitioned off from the rest of the cluster at the worst possible time:
%
\begin{itemize}
	\item A minister receives a write request for file F
	\item At time $T$ clerk A receives its last heartbeat from the President
	\item Clerk A loses connection to the rest of the cluster but stays reachable for clients.
	\item The lock manager fans out a lock request for F, it can not reach clerk A and starts retrying.
	\item Just before time $T+H$ clerk A issues a read lease to a client, it is valid until just before $T+2H$
	\item At time $T+H$ clerk A misses the next heartbeat and stops handling client requests
	\item Just before time $T+2H$ the client fails to refresh its lease and stops reading
	\item After $2H$ the lock manager gives up sending lock requests to clerk A. It is guaranteed that any outstanding read-lease issued by clerk A has now expired
	\item The minister issues the write-lease for F
\end{itemize}
%
We see that $2H$ after the lock manager started trying to lock the file it can assume the file locked. A clerk going offline increases file write access by $2H$. If the manager keeps trying to reach it we keep this $2H$ overhead. Instead, the manager removes the clerk before handling another request. Without any failure file write access time should be dominated by the latency of the TCP roundtrips.
%
\subsubsection*{Performance}
The lock manager has been written to handle many simultaneous requests. It is therefore lockless and open TCP connection to its clerks. The minister communicates with the manager through message passing. When clerk gets assigned by the president the lock manager receives a message. It then opens a connection in a new concurrent task dedicated to this new clerk. 

The decisions the lock manager makes directly impact the rest of the cluster. Each lock placed on a clerk potentially blocks read-leases which potentially slows down read performance. Therefore, it is important to unlock as soon as possible. The lock manager thus prioritizes unlock above lock requests.
%
\subsubsection*{Known problems}
The current implementation has four known problems. Three of these have simple solutions however third requires changes to the design. First, an imposter or failing node can still send unlock requests. Including the current minister term in the request and checking if its valid would solve this\footnote{Similar to \graft{}, see~\cref{sec:dictraft}}. 

Second, a newly assigned clerk can serve clients before it has processed all the existing locks. Clerks get their ministerial \raft{} log up-to-date before they start serving requests. The same should be done for lock requests. 

Third, a network fault could make it impossible for \textit{only} a minister, and thus lock manager, to reach one of its clerks. Traffic from the president and clients would still reach the clerk. In this case the lock manager assumes a file locked after $2H$ while the clerk does not miss a heartbeat from the president and stays up. This clerk could now enable reading to a file that is being written to. We can prevent this by making the minister inform the president of the clerk's failure. The president would then exclude the clerk from heartbeats triggering its shutdown on time. 

Finally, a lock request can fail when the file has not yet been created on a clerk. In \raft{} a log entry becomes committed after the majority has accepted it. In the current implementation file creation is done as soon as the corresponding log entry is committed. A clerk that is behind in processing log entries can receive and start processing lock requests. Unfortunately this reveals a design problem: \textit{there is no mechanism to handle a clerk lagging behind}. In section \cref{sec:fut} we discuss how the design can change to address this.
