In the last decade we have seen the rise of big data and the shift of everyday work to the cloud. Distributed processing of large datasets has been made easy with models such as MapReduce~\cite{mapReduce}. Distributed computing frameworks such as Apache Hadoop and Spark~\cite{spark} allows computations to scale across thousands of machines. With this rose the need for a highly available and scaling file system. This need is addressed by distributed file systems, they spread files across multiple machines and keep those replicas consistent. 

Files can get corrupted if multiple users write to them at the same time. The system therefore needs not only store the file data but also control access to the files. These tasks are normally done by two separate parts of the system: the data and control plane. The control plane is the bottleneck in file system performance. For this reason many systems allow client to opt out of access control and directly operate on the file. This can be useful in \ac{hpc} where many processes operate on the same file.

There are currently two approaches to distributed file systems: a dedicated node to control where each file is placed\footnote{Introduced by GoogleFs~\cite{GFS,GFS_interview} and adopted by the widely used Hadoop file system (HDFS)~\cite{hdfs}.}, or a distribution function that decide where a file should be located\footnote{Pioneered by \textit{Ceph}\cite{ceph}}. The distribution function needs only limited shared state however we still need a separate cluster to manage file access.

The first approach has two problems. The dedicated metadata node is single point of failure, and it can not scale out, limiting the amount of files a system can store and the maximum load. Recently implementations have started to address the first issue. \textit{HDFS} for example has added standby nodes \cite{hdfs_ha_nfs, hdfs_ha_q} which can replace the metadata node. However, ensuring consistency between the standby nodes decreases performance.

Here we investigate whether using groups of nodes for the control solves the scalability problem. We made \textit{Group Raft} an extension on the Raft~\cite{raft} consistency algorithm which enables highly scalable consensus when using groups of nodes that do not share an information. We use \textit{Group Raft} to design and implement \Name{}: a control plane that uses groups of nodes. 
%
%While the second approach scales and is failure resistant it gives up some control over where files are placed.
% adress why not just drop hadoop arch for \textit{Ceph} (ranged locks?)

When a hashing function is used to locate files (the second approach) metadata is stored in between the files and a cluster is used to control distinct parts of the file system. This solves the scaling problem. There are two reasons why we feel it important to explore the other approach:
\begin{itemize}
	\item Using hashing we lose some control over file location, this makes co-locating compute and storage more difficult.
	\item As the control plane often forms the bottleneck of the system we might want to optimize it using special hardware~\cite{polarfs}. When metadata is stored in the data-plane it is hard to do so.
\end{itemize}

In the next section we will go over the challenges in distributed systems, how consensus algorithms solve those, what a file system is and finally discuss the most widely used distributed file systems: \textit{HDFS} and \textit{Ceph}. 
%
Then in \cref{sec:design} we present \name{}'s design and explain how it enables scalable distributed storage.
%
In \cref{sec:impl} we go over the implementation, take a more detailed look at how we extended Raft and discussing how reading and writing is coordinated.
%
Using the \name{} implementation we performed a number of performance tests. We present the test methodology and show the results in \cref{sec:results}.
% 
We then discuss the results in \cref{sec:discussion}.
%
Finally, in \cref{sec:conclusion} we conclude whether \name{} approach, using groups of metadata nodes, offers better scalability. 
%
We finish by listing a number of interesting avenues for further study in \cref{sec:fut}.
