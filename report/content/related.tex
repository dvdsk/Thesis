% https://accelazh.github.io/storage/Multi-Paxos-Raft-Multi-Raft-Parallel-Raft
One of the key contributions of this thesis is \textit{Group Raft}. We use it together with subtree partitioning to scale up the control plane without the overhead that would come with scaling up a normal \textit{Raft} cluster. We know of no publication using Raft to scale up the control plane of a distributed FS. There is literature on using multiple raft groups and there is work on using Raft within a distributed FS.

\paragraph{TiDB} is a Raft based Hybrid Transactional and Analytical Processing database~\cite{tidb}. It uses multiple independent Raft groups to maintain consistency among replicas. To speed up reading a learner role is introduced. The learner does not use log replication nor does it vote in elections. The Raft groups leader pushes updates to the learner. When reading strong consistency is enforced between the leader and learner. In our \textit{Group Raft} we speed up reading by adding a Raft layer on top that takes care of heartbeats and elections.

\paragraph{PolarFS} provides a distributed file layer for cloud databases~\cite{polarfs}. It uses Raft to replicate file system chunks to three replicas. The replicas are only for redundancy and are normally not read from. Since modifications to different file system chunks do not affect each other they do not need to be serialized. PolarFS introduces \textit{ParallelRaft}\footnote{ParallelRaft has been formally verified~\cite{verify}.} it allows holes in the log making out-of-order replication possible. It makes it possible for followers to handle new log entries in parallel as they no longer need to ensure the previous entry is committed. The leader ensures only those log entries that modify different FS chunks are appended in parallel.

\paragraph{Clusterd Raft} is presented and proven safe in \textit{A hierarchical model for fast distributed consensus in dynamic networks}~\cite{craft}. It is designed to enhance throughput in Geo-distributed context. Each location runs a local Raft cluster, the leader of a location then replicates in batches to the other clusters. In contradiction to \name{}s \textit{Group Raft} their system still provides a global shared Raft Log. They report a 5x performance improvement in a globally distributed system when compared to Raft. We suspect keeping the log global comes at the cost of significant latency compared to \textit{Group Raft}.
